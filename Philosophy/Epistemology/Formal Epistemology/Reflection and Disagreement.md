How should we respond to those that disagree with us? Not only is there disagreement between different people, but there can be disagreement between a person at one time and that same person at a later time.

We'll look at two principles that aim to constrain beliefs given such disagreement.
1. The Principle of Reflection says roughly that your beliefs should match what you think you'll believe in the future. It says you should defer to your future self.
2. The Equal Weight View says that you should give the opinions of an epistemic peer the same weight as you give your own.

These principles seem to have initial plausibility, but face apparent counterexamples.

# Reflection
---
Generally, if you know what an *expert* thinks, you should defer to their opinion. But who counts as an expert? Van Fraassen made the suggestion that your future self counts as an expert. So if you know that in the future you will believe it will rain on Sunday, you should now believe that it will rain on Sunday.

Letting P1 represent the credences at the earlier time, t1, and P2 the credences at the later time, t2:

*Reflection*
If you are rational then $P1(H| P2(H) = x) = x$

Given that your later credence in H is x, your earlier credence in H should be x. You can understand reflection as expressing *self-trust*. You should trust your future self to form the right beliefs, and so you should defer to those future beliefs.

Reflection is saying that if the t1 agent believes that she will believe-H-with-credence-x-at-t2, then she should now believe H with credence X.

The purported counterexamples to reflection fall into two main classes: *belief in future memory loss* and *belief in future irrationality*.

## Belief in future memory loss
William Talbott argues that you should not satisfy Reflection regarding what you have for dinner because you'll forget.

Suppose at t1 you remember (S) having spaghetti for dinner on a particular day, so P1(S) = 0.999. By next year, you will have no idea what you had for dinner that day. Youâ€™ll only know that you eat spaghetti, say, 10 percent of the time, so P2(S) = 0.1. Reflection says you should defer to your future self and have 10 percent credence in S, that is, P1(S | P2(H) = 0.1) = 0.1. But this is obviously wrong. You should ignore your later forgetful state and hold: P1(S | P2(H) = 0.1) = 0.999. So Reflection is not a constraint on rationality.

It doesn't matter whether you really do forget. As long as you expect to forget you don't even need to fully believe that you will lose your memory. You should violate reflection even if you believe there is just a possibility that you'll lose your memory.

## Belief in future irrationality
David Christensen offers an example involving a drug, LSQ, that causes people to believe (F) they can fly by flapping their arms. So at t2, after taking the drug, P2(F) = 1, but knowing at t1 that you're about to take a drug, you should not believe you can fly, so P1(F| P2(F) = 1) = 0.0001, violating reflection.

There are many similar examples. See the book for more.

## Qualified Reflection
Despite these problems, there still seems to be something correct about reflection. Knowing this some philosophers have argued that reflection just needs to be qualified.

These two classes of counter-examples suggest two qualifications have to be made to Reflection. Agents should satisfy Reflection if and only if
1. They are certain that they won't forget anything.
2. They are certain that future changes in belief will be rational.

Notice it doesn't matter what will happen; what matters is what the agent thinks will happen. They should satisfy Reflection when they think they'll keep their memories and update rationally in the future.

(2) needs clarification. What is it for future belief change to be rational? We adopt the theory that says the only way to rationally change belief is to conditionalize. So (2) says that the agent is certain that all future changes in belief will be due to conditionalization. If so (1) follows, as forgetting is a way to change your beliefs that differs from conditionalization. So we can simply the condition:

Qualified Reflection Agents should satisfy Reflection if and only if they are certain they will always be conditionalizers.

Assuming this is correct, should we satisfy Reflection? Plausibly, we shouldn't, for we should not be certain we'll always be conditionalizers. We are flawed agents susceptible to drugs, cult leaders, hypnotists, and forgetting.

An interesting feature of this result is that it separates the question "what constrains should my credences satify?" from "what contraints does an ideally rational agent satisfy"? An ideally rational agent never forgets and is immune to drugs, cult leaders, and hypnotists, so an ideally rational agent satisfies Reflection. But that doesn't mean we should.

This is an instance of a general problem in arguing from ideals to constraints for humans. There are ways in which we should not emulate ideal agents. To be as ideal as we can, we have to make allowances for our flaws.

The motivation for Reflection was there seemed to be something right about saying that you should defer to your own future opinions. But what about deferring to the opinions of other people? That doesn't seem as appealing.

# Disagreement
---
Say a Rabbi and a Priest walk into a bar and try to convince the other on matters of theology. By closing time several things become apparent.
1. Each is rightly convinced that the other has all the same evidences, for example, they know all the same texts just as well.
2. Each is rightly convinced that the other is just as intelligent as they are

When these two conditions hold, equal evidence and equal intelligence, we say that the agents are *epistemic peers*. We'll also assume the agents know they're epistemic peers.

3. Neither will budge. The disagreement between them remains.

The question is, is it rational for this disagreement to remain?